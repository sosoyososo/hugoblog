---
title: '走向智能机器的路线图  (中文翻译 A Roadmap towards Machine Intelligence)'
date: Fri, 21 Oct 2016 08:10:01 +0000
draft: false
tags: ['Read Note', '人工智能', '翻译']
---

[原文地址](https://arxiv.org/abs/1511.08130)

1\. Facebook AI Research  
2\. 特伦托大学

**概述**

智能机器是当前计算机科学中的一大未解难题，本文假设一些智能机器应该具备的一些基础属性，并且着重论述_沟通_和_学习_。我们讨论一个简单的环境，这个环境用来增量的训练机器理解基于自然语言的沟通，并作为一个更复杂的与人类用户沟通的前置条件。我们还将提出一些机器从环境中高效学习的排序算法假设。

1\. 介绍
------

一个能够执行复杂任务而不用繁杂编程的机器，从帮助我们执行简单的工作到发展基础科研，几乎对人类的任何工作都非常有用。有了现有的硬件能力、大规模的机器可读数据、以及社会上广泛传播的对于机器学习方法的兴趣，发展智能机器的时机已然成熟。 但是，因为一次达成“solving AI”这个目标看起来仍然比较复杂，最近几十年，计算机社区更倾向于面向特定用途，解决经验上看，最重要但也相对更小的一些问题，而不是发展一般意义上的智能机器。这片文章中，我们提出另一种可能：首先我们定义一个我们认为的智能机器需要具备的通用属性，然后我们作出发展它们的切实可行的、小进步组成的计划，这些计划将组合起来，最终带领我们靠近实现强有力的AI的终极目标。 这片论文结构如下:第二部分我们描述两个我们认为对开发智能机器至关重要的基础特性（至少是我们对于智能最感兴趣的部分），称为_沟通_和_学习_。我们的目标是建立一个通过与人类交流来学习新概念的机器，并且与有着相同知识储备的人类有相同的效率。这就是说，如果一个人可以在掌握加法后，可以简单的学习的减法，智能在掌握了加法后学习减法时，同样不应该很困难。就像我们所说，一次性达到建立一个有着这样特性的智能机器的长期目标很困难，我们需要建立能引导我们正确方向的短期目标，我们通过简单但字包含的版本来明确我们最终想要开发的机器的各个目标。在它自我学习的任何时间点，目标机器都表现的像是一个独立的智能系统，即使它最初初始化的时候智能做很少的事情。因此我们提议的大头（第三部分）在于一个交流式学习系统的计划，这个计划帮助增量的开发逐渐更加智能的行为。第四部分简要的讨论一些我们认为的机器充分利用学习环境所应该具备的一些算法能力。最后第五部分在现在以及过去所有对发展人工智能尝试的广阔背景中定位我们的建议。需要澄清的是，我们计划中的许多想法是其他研究机构已经提出过的，我们相信如此的组织，是一个让我们的描述更加连贯的方式。

2\. 智能机器所需要的特性
--------------

相对于非常正式的定义智能，我们在此假设一些智能机器必须的属性，通过这些属性，智能机器能够自由通过自己的努力来让自己对人类更有用。创建这些特性是我们默认的指导原则是让机器最简单，最大化人类对它的影响。

### 2.1 交流的能力

任何有实际用途的智能机器都必须与我们进行交流，建立一个用来执行复杂任务但是我们却没有办法来明确任务目标活着理解输出数据的机器是无意义的。虽然其他的沟通技巧可能很好玩，但是自然语言是我们目前最简单也最强大的沟通工具，因此我们有必要要求智能机器能够使用语言进行沟通。实际上，我们所期望的智能机器可以被视为可以通过自然语言来进行编程，或者是自然语言与一般编程语言之间的接口。人类通过他们的自然语言编码了大量的知识（从数学论文到料理书籍），所以掌握了自然语言的系统访问大部分人类需要通过课程进行学习汇总的知识。 交流，它的本质就是_交互_：有能力维持一个对话对于获取新的信息（获取解释、声明、结构、反馈）和加速信息传播（对比一个好的课程、团体中一对一学习与独自一人看书）都至关重要，因此我们的学习环境将会非常着重于交流的交互本质。 自然语言在某种程度上也能转换非语义信息，因为大部分后者内容可以通过语义工具来表达。比如，我们可以使用语言来表达我们感觉到的东西，或者给出在世界中如何操作的说明（见2011年Louwerse给出的语言编码很多感觉方面知识的证据）。类似的，在我们后面的模拟中，一个Teacher使用自然语言来教Learner（要训练的智能机器）一种更有限制也更加详细的语言（就像一个简单的编程语言），Learner可以使用这种语言向模拟环境中提交指令，并且使用同样的语言跟Teacher沟通。这个智能机器可以在使用它的常用沟通方式接受一些命令后被用来浏览互联网，以此掌控一个强大的与世界互动的工具。语言也可以作为感知组件的工具，从而更新机器的物理环境。比如，一个对象识别系统可以将原始的像素数据转换成标签对象，以此来允许机器通过可控语言的方式"看到"它所在的真实环境。 另外，我们意识到我们向智能开发语言调节方面的偏向会限制机器自己发展自己的一些能力，这些能力我们人类可以通过观察周围世界轻易获得。就像我们所觉察到的那样，在语言的符号表达和世界本质之间似乎有一个基本的守恒原理。 如果确实如此，我们可以将机器的训练方式（他在模拟环境中的开发会就像下面描述）扩展的更加偏向感觉。在我们描述的人物中，机器将会被训练如何通过他的输入输出通道来接收传递语义符号，机器同时可以通过这个接口接收简单编码（二进制流）的连续信号，比如图片。机器可以因此被训练，首先可以理解连续数据的基础属性，然后可以在连续空间里执行更复杂的操作，比如在一个 2D 图片中辨识形状。请注意，加入这样的任务并不会改变我们的学习框架的设计，只是导入了新的训练脚本。 我们现在所追求的单接口的一大好处是机器只需要一个机遇二进制的输入输出通道，所以可以最大化的简化这个接口。机器可以通过相同的通道向其他所有的可以交流者（人类，其他机器，上面描述的感觉上的数据等）学习无限数量的这个接口可以接受的代码。给机器一个简单的输入输出的比特流接口，我们更加保证了机器不用浪费精力考虑输入输出数据的结构，危害了继续学习策略的通用性（对比处理一张图片时，编码成像素数据跟原始比特数据之间的难度）。 最后，虽然我们使用语言作为机器的通用接口，我们并不知道机器处理必须面对的事情时候的内部表示方法的实质。甚至，我们都不声明机器的内部基于可翻译的"思考的语言"的实现方式（1975年Fodor）。也就是说，我们并不限定机器必须用语言的方式对内部进行实现:只有输入输出是自然语言。 为了给出一个基于沟通智能机器如何有用的例子，考虑一个帮助科学家进行研究的机器。首先，能够沟通的机器不需要预编码关于参数的很大的统计数据库，因为它可以从互联网获取相关的信息。如果科学家问了一个简单的问题，比如：金的密度是多少？，机器可以搜索网络，返回结果:19.3g/cm3。 但是大部分问题需要机器综合多方面的信息。比如一个人这么问：研究强化学习的好的起点是什么？机器可能要访问多哥网站，搜索相关资源，然后看结果的流行程度。然后，可以做出一个简单的查询，比如更流行的更好。比如，机器可以问一个人她更喜欢视频还是文章，背后的数学原理是什么等。 对比上面，我们真正感兴趣的是一个可以通过解决类似如下问题有效加速研究进程的机器：解决癌症更有效的研究方向是什么？我应该如何开始一个有意义的捐赠？这种问题在机器读取了数量可观的线上文章之后可以回答，同时考虑到了提问者的感觉。交互在理解她的动机、技能、她愿意花的时间等主题上再次发挥关键作用，并且作为可能正准备跟请求者进行沟通的智能机器的最佳行为训练。在深入一点，为了满足上述要求，机器甚至可能需要自主的在线上对一些信息进行搜索，可能跟专家进行交流，然后找到研究者，使用多个交互过程，努力达到她要求的目标。

### 2.2 学习的能力

可以说，"good old"符号AI研究（1985年Haugeland）的主要缺陷就是对手写大部分智能机器的代码的可能性的假设。我们毫无争议的相信一个能在很多开发者都没有前瞻到的方面帮助我们的机器，应该有学习的能力。一个不学习的机器不能根据经验适应或者修改自己，因为在面对某种特定情况时，将在它的整个生命周期中都表现一致。但是，如果机器犯了我们想要它修正的错误，他就需要修改它的行为－也就是说学习是必须的。 伴随学习而来的是学习动机。学习允许机器适应外部环境，帮助他最大化的强化动机定义的功能。既然我们是为了开发可以让自己对人类更有有的机器，动机应该可以通过沟通渠道被用户直接控制。通过指定有正面左右的内容和有负面作用的内容，这种方式可以对机器的行为进行塑形，它可以因此而对特定任务变得有用（这很大程度上就是增量学习的精神，可以查看 1998年的"Sutton and Barto"或者后面第五部分的讨论）。 请注意，我们将会经常的引用人类学习作为洞察力的源泉和追求的理想标准。既然我们想要我们的机器发展的跟人类很像的智能，这就是很自然的事情。同时，孩子成长的环境跟我们训练机器的环境有明显的不同，孩子们能很快的发展出一个运动感知系统用来跟世界进行交互，并且他们本身就被赋予其他的很多认知能力。而机器不同，它们必须从头开始，所以它们不但需要学习人类的本体特性，还要学习他们的进化过程（AI的研究表明让机器从数据进行学习比手动直接编码"天生的"知识给它更有效）。好的方面是，机器跟生物组成的小孩不同，我们可以将它们暴露在对孩子来说过分严格的训练中。因此，相对于人类学习需要鼓励，我们没有必要让我们的机器向人类已让学习，我们在选用学习方法时不用管心里上的合理性。

3\. 教育沟通作为基础的智能机器的模拟环境
----------------------

这个部分里我们描述一个被设计用来训练智能机器基本的语义交互的模拟环境，以及怎么使用它来学习在世界中处理事物。这个模拟的生态环境应该被视为一个为智能机器提供基础教育的"幼儿园"。这个可控环境中被训练的机器，后续将被链接到真实的时间中用来帮助人类完成不同的任务。 这个生态环境的输入输出系统被一个自动化的系统控制，防止机器在最初的时候就碰到现实世界中产生的难题，并且让我们可以直接的聚焦在找到让新机器能够有效学习方法的挑战上来。 这个环境必须有足够的挑战性让机器发展出复杂的学习策略（尤其是他应该"学习如何学习"）。同时，复杂度必须可控，如果把人放入相同的环境中，即使这个环境中的语言是这个人还没有完全熟悉的，人也不应该对学习感到毫无理由的困难和不知道怎么面对。在掌握了想通环境中基础的语言和概念后，机器应该能够跟人类老师没有障碍的学习和交流。这就为机器必须有的学习方法增加了一些限制：最重要的是，它应该能够从少量的例子中概括信息，并且有跟人类学习者差不多的提取率。 我们生态系统的想法是从近几十年的AI研究中发展而来，经验表明系统应该直接面向真实世界的问题，以防他们被困一个实验属性跟现实世界有明显的不同的"自我世界"中（1971年Winograd）。我们的策略基于我们将在第四部分讨论的观察，当前机器的学习技巧不能处理那种真正的学习技巧，而这正是智能机器发展所必需的，因为它们缺少技巧进行长期存储并且对它们进行组合。为了达到这个目标，我们当然有许多选择，且很有理由选择一个最简单的。我们提出的系统充分展示了当前技术的缺陷，但它足够简单，让我们能完全控制我们要机器解决的任务的结构和实质，确保它有解决方案，并且可以用来促进新技术的发展。假设我们在一个更自然的环境中工作，比如视频输入，这将向开发者提出大量基础设施的要求，而数据处理本身就会是一个巨大的挑战，而即使是一个最简单的训练都会非常耗时，况且这将让相关任务的公式化和定义成功标准更加复杂。一旦我们用我们的生态系统花费少量大家来开发一个能够组合学习技能的系统，加入一个更加自然的信号的方式将非常简单，比如，通过跟真实人类交流或者从互联网获取内容的方式，而系统也因此可以学习怎么来完成用户真正想要它们完成的任务。 我们的方式跟传统自我世界的AI相比，明显的不同就是我们不愿意用我们的系统来对具体功能进行详细编码，而是教会机器一些基础的能力来学习怎么通过组合已有的技能来轻松的学习。一旦这样的机器和真实世界相连，它将能够快速的学习到它的老师为他选择的新的任务。我们的环境可以看成专业学校的等同物。学生在小学通过一些人造的问题来学习数学，但是一旦在这里他们掌握了基础的数学技巧，他们可以在真实世界中快速适应来结解决类似问题，并且以此来获取更加复杂的数学知识。

### 3.1 生态系统的高层描述

达成开发一个能够成长的人造系统的目标需要使用新的语言交互工具，我们不能像通常的机器学习中设置的那样，把训练数据作为带有标签的静态实例集合。相反的，我们提出一个类似电脑游戏的动态的生态系统，学习者（需要被训练的系统）是这个系统中一个角色。 系统中第二个基础的基础的角色是教授者，系统中的教师给学习者分配任务的奖励以让其作出理想的行为，同时教授者还自主给出有用信息并相应学习者的请求，而教授者的行为被实验员通过脚本完全控制。这同样会让会引起人们对手写 "good-old" 人工智能的担忧，但是其实教授者并不用设计的特别复杂。实际上，对于面向教授者的单一任务，它会存储一小部分可预计的反馈，并且仅当行为非常符合预计情况的时候才奖励学习者。类似的，教授者被限制在智只能反馈他所知道的一小部分反馈中。这种方式能够成功的原因是它的目的是让系统具有高效学习的能力而直接给它能够在世界中自我满足的所有知识。在具有有限已经编码的教授者的情况下，系统中的学习者将只需要自然语言的一个非常小的子集版本，同时学习者学到强大的能力和通用的策略。使用这个最小化的语义技巧和它获得的强大的学习能力，一旦接触真正的人类使用者，学习者就应该能够快速的扩展它在语言方面的知识。 跟经典的基于文字的毛线游戏（Wikipedia, 2015b）类似，我们的系统完全使用语言进行定义，并且被学习者通过给出结果、询问问题、接受烦哭（尽管图像没有在我们的模拟中发挥积极作用，但就像我们后面索要展示的，它对于观察2D世界以便更好的追踪学习者的行为有更直观的作用）来进行探索。因此，最好将我们的系统看作是生态系统中第三个基础角色，虽然系统的行为也是被定义的，但既然系统的作用是观察并引导学习者的周边环境（"动作感知经验"），定义系统的语言相比教授者就是可控的并且更加严格、专业、更少的二意性。学习者因此可以被看成是更高级的编程语言，从编程者（也就是教授者）那里获取简单形式的自然语言指令，转换成系统可以理解的机器语言。 在后面的例子中，我们假设由系统所定义的世界是由离散的单元所组成，并且学习者可以横向或者纵向的贯穿整个世界，这个世界中有障碍物，比如水和墙，也有一些学习者可以交流的物体（比如梨和被子）。 请注意，虽然我们没有暴露这种肯梦醒，但是在模拟中加入其它的角色也可能是有用的:比如同时训练多个学习者，并且鼓励他们相互沟通学习的，同时与编码好的教授者进行交互。 通用的收入输出方式定义学习者的交互渠道，教授者、系统、其它具有语言能力的角色进行输入，奖励（后面将会讨论的一个标量数值）也会进行输入操作（但是我们假设学习者不需要知道具体的哪个比特指的是奖励，因为他将需要使用这些信息更新它的目标函数）。每个角色的都会通过独特的前缀字符串对自己产生的消息进行标示来避免冲突（比如就像后面的例子所示，来自教授者的消息可能使用"T:"作为前缀）。学习者向自己的输出通道写入内容，并且它被使用类似的方式教会使用独特的前缀来定位教授者、环境和其它任何需要与之沟通的角色和服务。只要学习着能够学习沟通需要的语言，那么只使用通用的输入输出沟通渠道应该能够无缝的添加新的交流实体。 **奖励** 奖励可以是正向或者负向的（1/－1），后者可以被用来加速引导学习者离开死胡同和避免做出甚至是有害的行为。教授者与后期的人类用户通过控制奖励来训练学习者。我们也能让系统通过硬编码的奖励来反馈学习者来模拟对类似吃东西或者受伤的行为的训练。跟生物所处的真实场景类似，奖励是比较少的，只有当完成特定任务的时候才会发生。随着智能的增长，我们期望奖励变的非常少，能够完成复杂计划的学习者，只有当成功完成任务的时候才给予奖励，甚至显示一定程度的自我激励。实际上，学习者因该被教会短期正向奖励不应该上时间影响自己（比如存储低营养食物而飞去更远的地方寻找更好的食物）,有时候应该将没有任何好处的短期活动的奖励最大化（学习阅读可能非常无聊耗时，但对加速学习者通过自主搜索网络寻找有用信息来解决问题很有帮助，并随之而来的能快速增加奖励），更进一步的，当学习者"成年后"外部的奖励可以完全停止，将不再有新的外部动机刺激学习者去通过新的方式学习，但是理想情况下，它应该已经获得包括类似好奇心（看后面）在哪的机制，这些将带领它为了自己的利益继续学习新的技能。请注意，我们说奖励可以完全停止，我们的意思是用户可以不必通过简单的数值提供额外的奖励，但对于学习者来说，我们人类可以将这个阶段视作学习者已经在其内部实现了自己的奖励机制，不再需要外部的模拟。 我们使用二进制的奖励，如此人类用户可以不用担心给学习者的奖励程度（如果确实需要控制奖励程度，可以通过奖励的次数来控制）。学习者 应该在任何时候都最大化平均奖励，这样可以从不用的训练活动中获得不同程度的奖励（这跟通过强化学习来持续获得奖励是类似的概念，这有可能是一种对概念进行形式化的方式）。即使两种解决问题的方式获得了同样的奖励，更加快速的一个应该是更好的选择，因为它留给学习者更多的时间来获取其它奖励，这将自动导致智能偏好于更有效率的解决方式。更进一步，通过使用类似简单的时钟时间的方式来衡量解决问题每一个独立模拟步骤的效率，人们应该惩罚使用长时间进行离线计算的无效率学习者。 就像我们提到过的，我们这个基于奖励的学习方法跟增强学习有很多共同之处，实际上，我们的设置确实是符合增强学习问题的构想（Kaelbling 等人, 1996; Sutton and Barto, 1998）－看第五部分关于这点更深入的探讨。 **Incremental structure** 接着我们关于游戏的想法，当前期获取的技能需要用来完成后期任务时，将一个学习者看成是通过一些列的等级是很有用的。在同一个等级中，没有必要强制任务的顺序（即使在我们的直觉中任务是有执行的先后顺序时），我们应该通过多次循环执行着这些任务让学习者自己发现自己的最佳学习路径。 开始的时候教授者训练学习者执行简单的任务来训练语言沟通和发现简单算法。教授者首先在学习者重复字符的时候奖励它，然后是单词，分隔符和其它的控制语句，学习者逐渐被教会如何重复和使用更长的序列。在后续的任务重，教授者通过学习者将行为和语言表达联系起来引导它慢慢理解语言符号的语义。这个目标通过训练学习者重复代表系统命令的语句的练习来达到，并且只有当它意识到命令所具有的状态的时候（后面我们会展示具体的例子）才奖励它。在这个阶段，学习者将能够将语言和原始的行为动作（向左转）联系在一起。接下来教授者将指派任务序列（找到一个苹果），学习者应该将它转换成一个简单的命令集合（简单的"程序"）。教授者将会渐渐的限制自己指派一些抽象的任务（取回食物），但并没有具体的参考步骤，以此来激发学习者让其创造性的思考自己的行为（比如，如果学习者在寻找食物的时候遇到困境，可能会发展出绕过障碍的策略）。在学习解析和执行高层命令的过程中，学习者还应该被训练向教导者澄清问题（比如通过自发定为到教授者的时候获取初始的奖励，以及通过我们后面例子中描述的基于重复的策略）。当任务更加通用更加复杂，训练者的语言也变的更加丰富和模糊（在能够被编码的限制下）,以此挑战学习者处理限制标本常见的语言现象比如多义性、模糊性、重复性和量化。 为了支持我们在第二部分所展望的和我们将在本部分最后所讨论的场景，教导者最终需要教会学习者如何"阅读"自然文字，以便让学习者可以通过访问网络来自动在线的获取信息。顺便的，请注意，一旦机器可以阅读文字，它也就可以利用对大量文字的分布是学习（Erk, 2012; Mikolov 等人, 2013; Turney and Pantel, 2010）来引导我们解决之前提到的一些具有挑战性的语言现象中词语和短语的表达，比如如多义性和模糊性。 学习者首先必须先完成它前面需要的训练，这些严格的训练它完成简单的任务，比如组合基础的命令。但是，对于有希望发展成一个全功能智能机器的学习者，我们需要让"雪球"效应快速发生，这样的话，即使后面的任务更加复杂，但也只需要少量额外的训练，因为有了学习者创造性组合能力作为背景的组合爆炸（就像人类，学习上网时间总比学习说话要短）。 **超时** 在整个模拟阶段，我们能够预见学习者能自由的跟系统和没有一个固定任务的教导者进行交流。系统应该学会充分利用超时来在正在进行的训练中进行更好的无向探索，就像在玩游戏的快要死的时候，玩家更应该尝试各种可能性而非坐等奇迹发生，或者像是到一个新城市的时候我们更应该到处观光而不是坐在酒店里。因为好奇心在多种情况下都很有用，所以这种行为自然应该获取更高的奖励，从而导致学习者更爱学习。如果策略允许额外的空闲时间和计算资源，超时也可以被用做"思考"或者"打盹"，在这段时间里学习者可以重演最近的经历或者根据刚刚获取的更加全局的视角来更新内部的结构。 **评估** 学习者可以通过在一个固定时间段内完成新任务的数量来量化评估，这是一个"全时段最大化奖励"的在线评估我们给出目标的工具。因为具有交互性，多任务的系统设置不支持对训练和测试的自然识别支持，所以机器从一开始就必须小心的选择奖励最大化行为。于此对应的，仅仅评估最后的行为将会忽略它寻找解决方案所作出的尝试。这样的评估方式有利于模型简单的记住观察大量训练数据得到的模型。在许多领域中，这种方式挺好，但是我们对于能够学习真正通用问题解决方法策略的机器更感兴趣。随着任务变的越来越困难，基于简单的记忆方式的方式所需要的计算资源数量成几何倍数增长，因此只有能够高效的进行总结的机器才能在我们的系统中获胜。在后面4.3种，我们将更详细的讨论基于记忆的机器相对于学习算法机器的限制性。 我们更倾向于在一个开放的环境中通过使用我们的生态系统开发智能机器，就像我们说过的，竞争不会陷入只分配一组静态的训练/开发数据的困境，这种困境在本质上就像是一个终极的测试集合。我们能够预见的不是一个开发人员设置的一个短时所有预订程序的系统，对学习者的后续评估将使用与开发阶段完全不同的任务。测试任务与开发时任务不同的例子包括教导者使用新的语言、使用不同的系统地形、不同地形下新的障碍物、以及系统任务描述方面的努力（比如测试任务可能要求在没有接受贸易训练之前买卖东西）。

### 3.2 模拟的早期阶段

**初期** 在最开始的时候，学习者不得不花费精力从教导者那里学习如何分辨出语言的基础单元（在比特码中寻找规则，学习字符，然后时词语，然后这么一直增长）。此外它还必须获得序列重复和操作的技巧，然后发展出形成记忆和有效学习的技巧。这些初始阶段的学习尤其重要，因为我们相信这些构成智能的基石。 但是，因为比特序列并不易读，我们将焦点放在后面一个阶段，在这个时期，学习者已经学会来如何注意老师和如何操作字符串。我们展示了教授者如何通过交互式的沟通来引导学习者从只会这些基本技能到能够解决相对复杂的环境导航问题。因为我们在逐步获取高级技巧时设想的"碎片化"的结构，这些步骤将显现出很多我们可以通过第几的最初的例程来演示的共同点。我们所描述的任务也逐步的结构化，从学习者学习接受环境命令，然后注意到这些命令的影响，然后理为了抽象跨类别的操作和对象去解命令的结构，一直到能够处理高级命令。到这时，学习者的交互式沟通就被初始化成功了。 注意我们只说明了"礼貌的"交流，所有交流的信息不会重叠，只有当收到一个消息的结束符号后，另外一个交流者才开始想交流通道内写入数据。但是我们并不限制所有的交流必须有这样的限制，相反的，让交流者在它们喜欢的任意时刻想交流通道写入数据是有好处的：比如教授者会可鞥需要打断交流者执行有灾难后果的命令，又或者学习者会因为已经知道要做什么并且想要加速获得奖励而打断教导者（一个简单的优先级列表可以解决导致的冲突，比如教导者的声音比环境的"大"等）。 还需要注意的是，我们的例子的目的是为了说明一个大的样例中的遵循相似模版的一个具体例子，这个例子涉及到一些物体、障碍和可能的行为。此外给出的例子并不打算涵括生态系统中所有可能被实现的"learning-fostering"策略。最后我们再次强调，我们不考虑任务的严格顺序（不仅仅是因为一个客观上先天有序的学习模式无关任务难以解决），需要的是组织好的在统一水平的任务，这样学习者就可以循环的使用来发现自己最好的策略来解决它们。 **注释** 我们在左边显示学习者的输入（来自教导者、环境和奖励的消息，前缀分别是T:,E:,R:），学习者的输出在右侧（发送给教导者的消息前缀是@T:,给环境的是@E:）。我们假设同个语句中有多个前缀标识符的时候，后面几个不标示正在说话（比如第一次的交流中只有T在发言: "T: give order @E: I move." ）。句号作为消息结束界定符号（换行符只用做格式，被视作一个空格；同样的连字符也只为阅读方便需要被忽略）。我们用省略号（三个点）来在任意通道上表示我们不回复（比如学习者学习者在第一次找到正确选择前遍历了好多其他的解决方案）。最后我们在交流中会插入评论，用灰色显示。 **学习者学习发布系统命令** 我们从教授者交学习者产生系统命令开始，首先通过死记硬背引导到正确的收件人（@E）,然后鼓励它从教授者自己的自然语言切换到可控的系统语言。在这个阶段学习者并不知道奖励是什么（命令的语义），同样的一个好的学习者会注意到命令所触发的动作。下面是一个可能发生在学习阶段的额对话： [![111](http://www.karsa.info/blog/wp-content/uploads/2016/10/111-300x225.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/111.png) 学习者后面需要发展的技能包括对教导者的输入进行切分，生产对系统的分隔开的命令。起初的学习可能像下面例子一样简单： [![222](http://www.karsa.info/blog/wp-content/uploads/2016/10/222-300x165.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/222.png) 最终，学习着应该能够学会流畅的在跟教导者和系统的交流之间切换（因为两者使用不同的语言），将空啊拍大片的的语言消息转换成系统的动作命令（这种交流的例子看图一）。 **联系语言与行为** 接下来学习着被鼓励去注意系统中发生的事情，并且通过只有在学习者发出命令产生的行为在世界中能有效与性的时候才给予奖励将教导者的语言和世界的状态联系起来。一个作为例子的任务如图二所示，学习者不再能正确的将指令转换成环境中命令的时候，将不再得到奖励：命令也必须在世界中具有预期效果。在这个特例中，学习者因为受到阻碍所以命令执行失败。 **学习总结** 通过在上面实验中添加极限数据，学习者应该开始学习命令的组合性质（向左转向右转共享系统级别的特性，比如，它们都引起学习者视角的变化；只有当一个物体在学习者面前盒子里的时候，学习者才能捡起它）。接下来实验帮助学习者比短语命令和字符串记忆更进一步： [![333](http://www.karsa.info/blog/wp-content/uploads/2016/10/333-300x182.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/333.png) [![444](http://www.karsa.info/blog/wp-content/uploads/2016/10/444-300x171.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/444.png) 图一：如何让模拟被可视化来帮助开发者追踪学习者行为的例子。左边是学习者提交移动命令时系统的状态，右侧是命令执行后系统的状态。浅绿格子代表学习者的位置和它的朝向，这些格子看起来非常直观（最好带着颜色一起看）。 [![555](http://www.karsa.info/blog/wp-content/uploads/2016/10/555-300x128.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/555.png) 图二：教导者给出移动命令，向右转然后移动，但学习者在转到右边后被障碍物挡住，然后奖励就因为命令没有执行而延迟了。（最好带着颜色看） 下个例子中，学习者被要求捡起面前的物体，但却不指明是具体的哪一个（我们假设着饿简单明确的系统的语言没有表示某类物体的词语，所以学习者最终必须发现它必须自己为具体的物体命名）： [![666](http://www.karsa.info/blog/wp-content/uploads/2016/10/666-300x118.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/666.png) 正如我们所说，最初学习者将使用一个完备的搜索策略，列出所有它知道的物体，然后找出一个捡起。然后，教导者将教会它查看的命令，学习者之后应该能够发现一个比完全搜索更快的策略： [![777](http://www.karsa.info/blog/wp-content/uploads/2016/10/777-300x96.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/777.png) 通过课程学习新策略并不意味着学习者可以放心的去忘记它之前所学。比如在某些任务中看的命令并不管用（比如因为黑暗，学习者看不到前方物体）。在这种情况下，一个高效的学习者应该能够简单的重用之前学到的技能，比如全量搜索。 **理解高层级的命令** 接下来的一系列实验的目的是发展出能够将高层次命令拆分成基础动作的能力，显示固定的（移动两次），之后是灵活的（找到一个苹果）。通用的策略是提供一系列成对的实验。在第一个实验中，教导者给出的任务描述跟一个原始动作的序列类似，然后给出这个命令的高层次命名。第二个实验中，教导者只给出同样的高层次命名，如果学习者给出的行为跟第一个实验中类似，就对它进行奖励（我们都知道,许多复杂的语言表达式不服从简单的全局分析，看这里的例子, Boleda and Erk, 2015：我们将使用一些教导学习者如何将复杂概念分解成一步步的计划）。教导者可以从交迭代计数器开始： [![888](http://www.karsa.info/blog/wp-content/uploads/2016/10/888-172x300.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/888.png) 学习者应该最终能够不通过很多训练就能将修饰符有效应用到更深入的训练中（比如向左转三次）。 接下来可以展示如何将高层次的任务分解成原子操作序列。比如，为了找到一个苹果，学习者可以发展一个简单的策略向前走直到面前出现第一个苹果。教导者可以通过下面的技巧初始化学习者： [![999](http://www.karsa.info/blog/wp-content/uploads/2016/10/999-300x190.png)](http://www.karsa.info/blog/wp-content/uploads/2016/10/999.png) 注意这样一个例子对学习者理解我们所说的寻找一个苹果并不是极限，这个命令可以有多个解释：也许我们知识向塘它执行两次所给命令。因此，应该有多个长度可变的序列来表明这个任务真的是用来执行一个直到循环，也就是循环移动和查看命令直到学习者需要搜索的东西被发现。 跟高级的任务可以被定义成技能的组合，类似拿一个苹果，可以背拆分成第一，寻找一个苹果，然后捡起它。另外一个泛化是让学习者搜索多个物体。系统添加上障碍会让任务更有挑战性。之前一个阶段中简单的额向前走直到一个物体被找到将不再有效，因为学习者会在遇到第一个障碍的时候停下来。我们可以期望学习者独立发展出涉及到转弯的更好的搜索策略，或者我们添加更深入的例子来让教授者向学习者演示，如何通过更加直接的监督来提升能力。 **互动交流** 类似无方向寻找一个苹果这样的任务可能会需要长时间的随机搜索，因此我们需要启动互动交流，让学习者可以被教导者（最终被人类）更有效的指导找到正确的格子。在第一部分的实验中，学习者因为重复老师发出的一个如何做什么的命令（将它通过@T:前缀返回给教导者）的到奖励，紧接着作为对请求的响应，教导者产生一个精确的指令： !\[对话8\]（./AAA.png） 类似这个例子的实验后续将被需要使用随机搜索来完成的任务多次使用，但是开始使用怎么做某事的请求，然后教导者给出精确方向的方式可以有效的加速奖励。 **算法知识** 一些上面阐述的任务需要对了解基本的控制流结构。比如，解析一个动作修饰词意味着一个简单的计算形式，为了找出物体，学习者必须实现一个直到循环（程序上等同于 while not）。类似的，走出草地的命令也调用一个while循环。有效的完成更高级的命令，比如回家，以为着需要发展出更佳复杂的算法，比如路径寻找。在得到一个类似的算法库之后（可能被机器使用与编程语言完全不同的自己内部的表示形式进行编码），在模拟的高级阶段学习者应该能够有效的将它们组合使用来完成一些成熟的任务，即使这些任务是一系列子任务多层组合而成的复杂任务（找到愿意用两个苹果交换一个香蕉的人）。 就像我们在3.1部分所讨论的，学习者的功能本质上可以解释为学习如何通过教导者给出的描述来组合程序，产生系统能够理解的非常简单的命令的程序可以被理解成一系列的CPU。从这点来看，我们作为目标所训练的智能系统就是训练者（后面被人类的操作替代）和只能理解有限操作指令、需要为每个任务进行编码来操作的传统计算机直接的桥梁。因此我们相信，一个训练成功的智能机器可以自动组合电脑程序，而这很有可能在将来通过使用自然语言的交流来实现。

### 3.3 跟训练过的智能机器进行交互

对我们的计划做个总结的话，我们提供了一个激动人心的例子来演示一个教授智能机器的学校，后面会发现它在真实世界中非常有用。我们考虑这样一个例子，机器作为一个独自生活的老人爱丽丝的助手。Bob是爱丽丝的儿子，并且也跟机器进行沟通。 我们假设，作为训练的一部分，机器已经学会了如何提交网络命令和处理返回结果。在示例对话中，我们给出了一个通用的想法机器可以用来跟网络交流，而不是定义交流的精确语法。最重要的是，例子中的网络查询表明机器不需要为了完成职责而存储所有它需要的知识，因为它可以根据需要从网络获取有用信息，以及可以这样做的原因。 !\[对话9\]（./BBB.png） 基于沟通的智能机器应该能够使用一系列没有明确指令的任务，如果需要，用户可以给他进一步明确积极或者消极的奖励来激励它改变自己的行为。极少需要这么做，因为机器应该在已经掌握了很好的沟通能力之后才发给最终用户，并且后续的发展绝大多数应该通过语言。比如，当用户说不，不要再这么做，机器应该明白重复同样类型的动作会导致负面奖励，然后即使没有奖励机器也会修改自己的行为（同样的，另一种达到同样目的的方式是机器把相似的语言符号转换成内部的负面激励）。 只能机器的任务非常多变:除了我们刚刚考虑的日常生活助理，还可以是帮助学生完成家庭作业，从网络搜集信息帮助进行药物研究（还可以看上面2.1部分的例子），在电脑程序中寻找bug，甚至可以自己写程序。只能机器应该像现在的电脑扩展我们的记忆一样扩展我们的智力，这可以帮助我们完成今天不能完成的智力相关的任务。 我们意识到我们所致力构建的智能机可能成为一个强大的工具，并被用于可疑目的（任何高可以都有这种问题，包括飞机，火箭，电脑）。我们相信广受欢迎的电影扭曲了人们对于人工智能的额认识，预期考虑及其因为它们自身的原因接管整个世界，我们认为我们终将发现人工智能知识一额高工具：一个扩展我们能力帮助我们解决复杂问题的机器。况且，以现在的科技水平，任何对人工智能"是否友好"。我们预计即使不需要数十年至少也需要几年的发展，只能机器才能与人类形成竞争，这将给我们足够的时间来考虑潜在的威胁。

4\. 开发智能机器的建议
-------------

本部分中，我们将概述我们关于如何构建智能机器的想法，而这些想法受益于我们所描述的学习系统。虽然我们还没有一个具体的计划来构建这样一个机器，但我们将讨论一些为了支持这些理想功能所需要的一些属性和组件。并不期待完整的智能，我们只是想要一些精神食粮。就像之前所说，我们试图保持机器的复杂度在最低标准并且只考虑一些完全必要的属性。

### 4.1 学习的类型

有许多我们称之为学习的行为类型，先讨论它们十分有用。假设我们的目标是构建一个作为两种语言翻译者的智能机器（我们带给翻译任务一个基于词语的可能性）。首先我们在我们的模拟系统中教会机器一些基本的沟通技巧，然后我们通过例如不同词语如何翻译来开始教他。 这里有不同类型的学习行为发生。为了掌握基本的沟通技巧，机器需要理解积极和消极奖励的概念，然后发展出复杂的策略来处理新的语言输入。这需要对算法和记忆事实、技巧甚至是学习策略能力的发现。 然后，为了完成翻译任务，机器需要存储词语对，词语对的数量是未知的，一个灵活的增长机制是必须的。然而，一旦机器理解如何使用实例填充字典，剩下要做的学习就非常简单自然：机器不需要更新自己的学习策略，只需要用之前获得的技能将进来的信息记忆整理到长期记忆中。最后，一旦机器完成词汇记忆过程并开始翻译工作就不需要更深入的学习了，它的功能会自我完善。 机器的功能越是具体狭隘，需要的学习越少，对于十分具体的行为模式，应该可以直接编程解决。但是当我们的角色从类似简单的词语翻译、计算器、象棋玩家等到有着开放式目标的机器，我们需要更多的依靠从有限例子而来的通用学习。 可以看到当前最先进的机器学习状态在机器学习的层级中处于中间层次，类似自动语音识别、图形对象分类和机器翻译已经很难纯粹通过编程来解决，目前最好的系统依赖于某种形式的统计学习，它所使用的手动编码模型的参数则是从大量实例数据中估算。然而当前最先进的机器学习的能力十分有限，机器功能的额适应性十分有限。比如一个语言识别事情将永远不会在被要求的时候执行语言翻译指令 -- 一个人类成需要需要手动实现额外的模块。

### 4.2 长时记忆和组成的学习技巧

我们看到一个特殊类型的长时记忆是智能机器的核心组件，这个长时记忆应该能够存储学习技巧相关的事实和算法，并且随时可以读取。实际上甚至是学习能力也应该能够作为一系列的技巧存储在记忆中。当当前情况需要某些学习技巧的时候，它们应该在内存中组合成一个新的结构，所以机器应该能够对自己进行扩展。如果不能存储之前学习到的技能和事实，机器就不能处理一些简单的任务，比如回忆某个任务之前的解决方案。并且，经常发生的情况是一个新的任务跟之前的任务是有联系的，考虑下面模拟系统中的任务序列： • 找到并捡起一个苹果; • 将苹果带回家; • 找到两个苹果; • 找到一个苹果两个香蕉，然后将它们带回家. 解决这些问题需要的的技巧包括: • 在当前位置周围进行搜索的能力; • 捡起物体的能力; • 记住家的位置并返回的能力; • 理解一个和两个的能力; • 组合之前（也许更多）的技巧来处理不同请求的能力. 前四个能力是一些简单的需要被记住的简单事实：一系列符号组成一些东西、一些完成一个特定行为的步骤等。最后一个例子具有创新能力，能够将已有的事实和技能组合起来的能力。因为有了这些学习技巧，机器常常能够快速将多个已知的技巧合并成一个新的技巧。通过这种方式，一个训练好的智能机器只用一个新功能的简单例子，而不需要大量新的实例进行训练就能完成新的请求。比如，当一个教导者要求学习者找到一个苹果两个香蕉把他们带回家，如果学习者已经理解了涉及到的所有简单指令，它就能获取相关指令，将它们进行组合，然后一步步执行。教授者甚至可以做出使用新获得的技能来准备早餐或者后面再使用这个技能等行为。理解这个新的概念应该不需要对学习者进行任何训练，后者应该能够长期存储新的技巧和与之对应的标签。 就像我们在之前例子中所见，学习者一旦掌握长期存储组合结构的技巧，就可以在不需要认为介入的情况下持续扩展自己的词汇储备、命令和技巧。我们可能会发现一些我我们通常认为理所当然的基础学习技巧比我们看起来更加错综复杂。但是一旦我们能够构建一个能够根据输入信号有效对自己进行重新组合的机器，即使没有明确的监督奖励，就像我们之前所讨论的，我们将能够距离开发一个智能机器更进一步。

### 4.3 智能机器的计算属性

智能机器另外一个值得讨论的方面是机器所依赖的计算模型。我们相信这样的模型应该是没有限制的，可以代表任何形式的数据。人类能够没有明显局限的思考、讨论算法（虽然对人类来说它们可能需要纸笔等外物支持）,一个有用的智能机器也应该如此。 我们所生命的计算理论中一个更加准确的构想是智能机器需要在一个图灵完备的计算模型。也就是说他应该像图灵机一样可以在有限步骤内完成任何算法（人类可以描述图灵完备系统的事实表明他们在实践中是图灵完备的：对我们的目标而言，人类的实时处理能力是否严格的图灵完备是不重要的，重要的是当有外物帮助的时候，人类的推理能力是）。需要注意的是，已经有许多图灵完备的计算系统，并且图灵机确实比很多替代品效率更低，比如随机存取机。因此我们对于围绕图灵机概念构建智能机并不感兴趣；我们只是想在表示模式的时候使用没有明显局限性的计算模型。 一个比图灵完备更弱的系统不能使用数据有效的表示一个特定的模式，也就是说他不能在通用意义上进行学习。然而它可以记住并非极度复杂的复杂模式，因此，即使是计算上受限制的系统在接受一些实例的训练后，也可以按照预期准确度完成相对复杂的任务。 比如，我们可以考虑序列重复问题。机器被期望记住符号序列之后还能对其进行再生，此外我们假设机器是基于具有有限状态机表示能力的模型。这种机器没有存储和再现序列的能力，但是，如果我们不完美的设计我们的实验，它似乎可以这么做。假设将机器看作是训练数据，并且这部分数据跟我们用咯哎评估机器性能的测试数据之间有明显重叠。一个能够查询表格的普通机器可以通过对训练实例进行排序和调用来工作。使用武术的训练实例，基于查表的机器看起来可以学会任何规律，它将与一个有真正有着重复概念的机器毫无二致的进行工作；但是他讲需要无限的资源。很显然，基于记忆的系统在我们的配置下不能很好的工作，因为我们的目标是使用少量实例来测试学习者概括的能力。 因为有着许多的图灵完备系统，有人会想那个作为机器智能的基础最好。我们现在还不能回答这个问题，但我们假设最自然的选择是一个并行计算的系统，基于手头的任务使用可以增长的基本单元。如果假设基础单元是无限个，增长属性对支持长期存储是必要的。具有许多所需特性的现有计算系统的示例是冯诺依曼的单元自动机（Von Neumann 等人 （1966））。我们也可能受到字符串重写系统的启发，比如L系统的一些版本（Prusinkiewicz and Lindenmayer, 2012）。 一个明显的替代方案是使用有巨大容量的非增长模型，但两者之间有重大差别。在一个不断增长的模型中，新的单元可以连接到那些产生它们的模型，所以模型自然能够基于功能连接发展出有意义的拓扑结构。我们推测，这种结构本身对学习的促进有着关键作用。有趣的是，现在有了一些更有效的学习方法，比如循环和卷积神经网络，它的特征（被人为的约束）是适合所在的特定领域的网络拓扑。

5\. 其他想法
--------

当然，图灵开创性的工作对我们有很大的帮助（1950）。请注意，虽然图灵的论文当作"模仿游戏"来引用，但是其中包含了非常有意思的想法，值得被读者更好的关注，特别是在学习机器的最后阶段。图灵认为一个构建出能够通过他著名测试的机器好方法是开发一台子机器（译者：技能比较少的机器），然后通过不同的沟通方式教会它起它技巧。这可能包括塑造子机器行为的稀疏奖励，以及其他信息丰富的方式，比如来自教导者的语言输入和感觉信息。 我们分享图灵构架一个能够使用自然语言进行独立沟通的子机器的目标，我们也强调稀疏奖励的重要性，图灵和我们的愿景之间主要的区别是图灵假设子机器的行为大部分可以编程实现（他估计需要六十个程序员工作五十年）。我们则更偏向于考虑从只有最基本的额技能的机器开始，然后专注于将学习能力作为需要开发的基础技能。这进一步的假设首先在模拟环境中通过智能教导者训练机器，正如我们在路线图中所概述的。我们相对于模拟游戏也存在分歧，因为我们的构建智能机器不是为了让人类相信她是一个真正的人。相反，我们的目标是开发一个机器，用来执行与人们通过计算机连接互联网、进行沟通像类似的任务。 最近人们开始对测量计算机智能的任务感兴趣，这是由于类似多层神经网络（LeCun 等人, 2015）的强大的机器学习架构的经验进步以及Turing测试的经典版本的明显不足所（Wikipedia, 2015c）推动的。比如，Levesque 等人 （2012） 建议测试系统在解决双关语歧义上的能力（奖杯不能放在棕色的手提箱，因为它太大了...是什么太大了？），Geman 等人 （2015） 提出一个“视觉的”图灵测试，测试中要求一个计算系统回答一系列关于图像中物体、属性和关系越来越具体的问题（蓝色区域内是否有人？这个人是否携带东西？这个人是否与任何其他对象交互？）。我们与类似的行为不同，它们专注于一组特定的技能（就像图像解析）而不是测试角色是否可以学习新的技能。此外，这些都是传统的评估标准，与我们提出的学习／评估混合系统不同。 开发一个在受控的合成环境中存活的AI，使用自然语言与其他角色进行交流，这样的想法由来已久。Winograd 的 《The Blocks World of Winograd》可能是这方面早期研究中最重要的例子。当事实表明，这个框架内发展起来的智能机器角色并不能应对真实世界的挑战时，这个方法被放弃了（Morelli 等人, 1992）。因为它们的学习能力非常有限，被这些早期模拟所测试的编码在系统中的知识主要是由它们的创造者手动编码实现。因此扩展到真实世界的时候需要手动编码所有需要的知识，而这被证明是不可能的。而我们的系统的目标则是预编码及少量的知识，但有着强大数据学习能力。重要的是，我们的计划并不打算像一些经典AI系统那样尝试在我们的系统中手动编码所有可能需要的知识。我们计划只编写出是环境，启发机器学习和适应不同问题和场景的能力。有了模拟环境之后，扩展学习着的功能将不再需要手动编写脚本来适应新的情况，反而更加需要现实世界的输入，比如来自于人类用户的。这个模拟世界自身已经被设计的有任务复杂度逐渐增长的特性，明确的测试了系统自我扩展的能力。 我们不会低估合成模拟的缺点,模拟环境中的一些任务可能会直接解决AI开发中一些具有挑战性的问题，比如在弱监督下学习能够形成结构化的长时记忆和在面临新的问题时子机器在大小和复杂度上的增长能力。但是，对真实世界的模拟只能将我们带到这里，我们最终可能是相对其他高估了某些随机现象的重要性，而这些现象在自然环境中可能更加常见。将真实带入我们的模拟世界可能非常重要，我们的模拟世界应该让智能机器发展到能够向人类学习跟人类交流，跟人类的交互将最终带领他解决现实世界的问题。在受控环境中训练的机器何时能够走入人类世界的问题是开放的，并且应该根据经验进行探索。但与此同时，我们相信在此之前我们让机器与人类进行交流没有意义，并且甚至可能会强烈的误导它。 我们的智能机器跟其他一些类似苹果的Siri和微软Cortana的当代人类助手有着类似的功能。但是，这是一些过度设计的系统，旨在为人类用户提供自然语言界面执行各种固定任务（类似 Tamagotchi 的人工伴侣和宠物也是如此， 参看 Wikipedia, 2015a）。这些系统定义了一些最常用的用户场景，可以使用当前技术解决（比如订一张机票，查看天气预报，设置明天早上的闹钟），并且实现了每个场景的解决方案。我们的智能机器不只是用来完成这些固定的任务，就像3.3部分中的例子所展示的，机器应该能够只通过跟人类用户进行交流（不需要程序员或者机器学习专家参与）来高效的学习到如何完成当前语音助手所能处理的任务。 软件架构，特别是智能系统的架构在AI及其相关领域被广泛的研究（Nwana, 1996; Russell and Norvig, 2009），我们不能查看所有的文献来跟我们的计划进行对比，简单的说我们并不是特别清楚其他以学习和交流作为中心的架构跟我们之间的差别，沟通在多角色的系统（Shoham and Leyton-Brown, 2009）中有着关键作用。但是本研究的重点是如何解决冲突和简单角色（大部分是编码好的）大规模组合的分布式问题。例如，交通模型就是一个经典的多角色系统应用场景，这与我们通过语言交流训练一个能够独立处理非常复杂的任务的角色的侧重点差别很大。 Tenenbaum （2015） 与我们一样强调聚焦于形成智能的核心能力，但是他使用单纯的物理问题作为起点，讨论特定形式的概率模型，而不是一个通用的学习场景。我们和Luc Steels的研究计划也有一些相同之处，他们让机器人通过交谈经过特定环境，以此来推演词汇和语法结构，但是一方面，真实机器人从一开始就要受到硬件和复杂的自然环境所需要的导航的限制，另一方面，模拟的重点在于对语言的学习，并没有面向进一步开发智能机器的目标。 我们与语义解析有几个联系点，比如人造世界中（MacMahon 等人, 2006）的导航任务和基于奖励的自然语言指令学习（Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013）。这个领域中开发的智能系统可以处理的任务类似学习执行通过跟人类进行交流而来的真实世界的指令（Thomason 等人, 2015），通过参考使用手册提升视频游戏的性能（Branavan 等人, 2012），这些任务我们也希望我们的智能机器能够处理。然而当前基于语法解析的系统之所以能够达到这样高的程度是因为随时根据特定任务修改程序的架构，所以她们依赖于一个同等数量水平的硬编码知识，特别是在语言结构上（虽然最近的工作朝着更少硬编码知识的方向发展，例子看Narasimhan 等人, 2015，通过只输入文字描述和游戏奖励玩一个基于文字的冒险游戏来训练神经网络）。我们的框架也能发展出执行类似任务的系统，但却是从只有模拟环境中学习的基础功能以及如何创造性地组合这些功能以达到更有意义的目标、基本没有预存知识的地步开始，逐步抵达这个目标。 过去的二十年我们见证与学习如何学习（Thrun and Pratt, 1997）、终身学习（Silver 等人, 2013）相关的几个提议，大部分的都是理论性的，而且重点是算法而不是对提出模型的实证挑战，不过它们所追求的大部分目标跟我们的计划是一致的。Ring （1997），需要特别提出，定义了一个持续学习的系统，它逐步经历任务，解决前一步任务所学到的可以后续使用，可能会用于完成一个完全不同的任务。Ring对持续学习的描述显然是跟我们类似，它是一个自动化的系统，它可以感知，采取行动对所在环境对它作出的奖励采取行动；它在完成任务的同时学习行为和技巧；它逐渐的学习，没有固定的训练集合；任何时间点学习动作都在发生，现在学习的技巧之后可以使用；它分层的进行学习，现在学习的技能，可以进行在此即去唱进行架构和修改；它是一个黑盒，内部的内容不用理解也不需操作。所有的行为都通过训练发展而来，不是通过直接操作它。它对接世界的所有接口就是自己的感知、动作和奖励。它没有终极形态，最终任务。现在学习的，后面可能有用也可能没有，取决于所要执行的任务。我们的计划跟这个十分类似，但更强调互动。 Mitchell 等人 （2015）讨论了NELL，它是终身学习架构的最充分体现者，它已经通过"阅读网络"多年来获取大量知识储备，重点在于永不停止的任务，基于NELL所学对持续的进化，以及跨任务的信息分享。最后一方面上，它类似于聚焦跨任务分享参数的多任务学习（Ando and Zhang, 2005; Caruana, 1997; Collobert 等人, 2011）。我们计划中成功的学习者像是也有类似的阶段，但是我们目前的焦点在于定义任务，而不是完成它们。 Bengio 等人 （2009） 提出于通过课程进行学习相关的观点，其中单个任务的训练数据根据复杂程度进行排序，以此希望能够更好的学习。这是观察人类逐步学会复杂技能而来，这个想法之前在Elman的复发性神经网络训练（1993）中也被研究过。持续学习的原则也是我们计划的核心，但是我们的基础部分不是严格遵守特定任务训练数据的顺序，而是智能机器应该发展出技能的持续增长性。对可组合任务的设计反过来应该能够促进者中增长性，因此这些从简单任务获得的技能将能更高效帮助解决更复杂的问题。 增量学习的方法，由与我们刚刚提及的论文中相同的想法所驱动，也出现在所罗门夫（2002），这项工作在程序归纳方面早有根基（Solomonoff, 1964, 1997; Schmidhuber, 2004）。有了这个先例，Schmidhuber （2015） 查阅了大量文献，提出了一些关于学习的想法可能会启发我们找到新的算法。基因程序（Poli 等人, 2008）也是通过聚焦于对已知方法的重用来加速寻找的效率。我们的想法跟Bottou （2014） 关于组合的机器学习的愿景的也有联系，尽管他所考虑的只是在特定范围内进行组合，比如语法和图形处理。 我们的想法与增量学习框架（Sutton and Barto,1998）有许多相同之处。增量学习框架中，系统中的角色会选出一些行为来随着时间让奖励累计到最大。增量学习在解决其中角色只能通过跟系统交流获取信息的问题时非常受欢迎。考虑到这个定义的广泛性，我们的框架可以被视为它的一个具体的例子，但却是与标准的增量学习（Kaelbling 等人, 1996）在多方面都有着明显的不同之处。尤其是我们强调语言作为媒介，交互沟通，我们聚焦在使用递增的阶段鼓励学习者使用之前学到的知识完成任务，我们以限制学习者完成固定任务所需要的额训练量作为目标。 Mnih 等人 （2015） 最近提出一个单独的神经网络模型能够学习一系列经典的Atari游戏而只是用像素和游戏得分作为输入（也可查看“通用游戏玩法”的相关想法，例如 Genesereth 等人, 2005）。我们试图通过低级的输入流和奖励达到类似的目标，但与这些作者不同的是，我们的目标不是一个不相交的同时学习多个任务的框架，而是一个可以基于之前执行的任务获取的技能逐步增加来处理更复杂任务的框架。并且，在奖励的帮助下，我们更强调语言沟通作为提升技能扩展的基础工具。Sukhbaatar 等人 （2015） 引入了一个沙盒来设计游戏，这么做的目的明确就是在计划和推理任务中训练计算机学习者，此外，他们还强调课程的策略对学习的促进（使角色通过越来越难的游戏）。他们的程序与我们的很一致，沙箱有可能能帮助我们开发我们的环境，并且他们持续学习的方法是用的是相同任务的不同难度版本（例如，增加障碍的数量），而不是定义难度逐渐增加的任务，比如解决后续任务需要对前面任务的解决方案进行组合，就像我们所希望的。并且，沙河中当前的任务并没有足够的难度需要学习新的方法，是可以被当前的已有的技巧或者它的简单变体所解决的。 Mikolov（2013） 对基于增量任务的方法做了一个初步讨论，我们则是更全面一些。基于类似的精神，Weston 等人 （2015）提出了一系列基于合成故事的问答任务，他们还希望促进人工智能的非渐进式进步，但他们的方法跟我们的在一些关键方面有着差别，再次没有互动和语言介入的学习概念，训练／测试被强制分开，并且任务的设计并不鼓励进行组合（虽然Weston和他的同事们强调同一个系统应该能被用于所有的任务）。最后她们的评估指标跟我们也显然不同 － 我们的目标是尽量减少完成任务是需要进行训练的数量，而他们的目标则是取出的数据的良好表现。这可能是涉及到人工任务的严重缺点，因为在我们看来牧鞭应该是开发一个机器，能够尽可能快的学习，有希望能够进行扩展摒弃可以通用的处理一些更复杂的人物场景。 有人可能会考虑解决序列操作问题，比如使用机器学习易经获得的技能的简单扩展来组合基础的学习例程（Graves 等人, 2014; Grefenstette 等人, 2015; Joulin and Mikolov, 2015）。就像之前部分所讨论的，对于只涉及到很小的有限配置的任务，很显然可以仅仅使用所有输入输出可能性组合起来的一张查询表我们即可成功。这种方法加上长时记忆（比如一系列的栈），对于数据的学习算法比较奏效，但他们使用它只存储数据，而没有存储学到的算法。所以，这种方法在通用环境中遇到需要组合学习到的算法来合成新的解决方案时就失效了。 类似的批评也出现在一些方法中，这些方法试图通过一个非常针对已发现信息的框架来学习特定的算法，但不支持哪怕是很小的改动。从我们的工作中给出一个例子：一个堆栈式的循环神经网络可以形成简单的长时记忆，可以学习记忆和反向重复语句，但不是最初的方向（Joulin and Mikolov, 2015）。我们期望有一个面向算法学习挑战的有效方案，利用少量的训练实例，尽可能增速的学习任务，并且使用越来越少的实例来掌握想对较新的技能。我们没有发现任何现在的技术解决这些问题，这些问题也是为什么Mikolov （2013）最初提出算法任务的确切原因。我们希望本文能够激励我们发展人工智能所需要的真正创新方法的设计。

6\. 总结
------

我们定义了智能机器的基本需求，强调学习和沟通作为它的基础能力。与当前普通的机器学习不同，我们的焦点在于在孤岛中创建单独的技能的墨香，我们相信智能的所有方面应该在一个独立系统中全面的得到解决。 我们提出一个模拟系统，它需要职能机器通过沟通获取新的技巧和事实。在这个系统中，机器必需学习处理越来越复杂的任务，人后能够自然的发展出复杂的语言系统和相应的能力。 我们还对智能机器可能基于的属性提出了一些猜想，这包括通过少量问题，在没有强监督的情况下进行算法模式学习，发展长时记忆存储数据和学到的技能。我们试图把这些与目前的机器学习范例进行比较，以此表明当前的方法有很大的差距，所以我们必须努力开发费增量新技术。 这个路线图只是走向AI长途的开始，我们希望其它研究者也能加入进来一起向这个目标努力。

Acknowledgments
---------------

We thank L ́eon Bottou, Yann LeCun, Gabriel Synnaeve, Arthur Szlam, Nicolas Usunier, Laurens van der Maaten, Wojciech Zaremba and others from the Facebook AI Research team, as well as Gemma Boleda, Katrin Erk, Germa ́n Kruszewski, Angeliki Lazaridou, Louise McNally, Hinrich Schu ̈tze and Roberto Zamparelli for many stimulating discus- sions. An early version of this proposal has been discussed in several research groups since 2013 under the name Incremental learning of algorithms （Mikolov, 2013）. 感谢 L ́eon Bottou, Yann LeCun, Gabriel Synnaeve, Arthur Szlam, Nicolas Usunier, Laurens van der Maaten, Wojciech Zaremba 以及其它来自 Facebook AI 研究团队的成员，以及 Gemma Boleda, Katrin Erk, Germa ́n Kruszewski, Angeliki Lazaridou, Louise McNally, Hinrich Schu ̈tze 和 Roberto Zamparelli 许多关于模拟环境的讨论。于此相关的初步想法早期以增量学习（Mikolov, 2013）的名义在许多研究团队中进行讨论。

引用
--

Ando, R. and Zhang, T. （2005）. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 5:1817– 1853. Artzi, Y. and Zettlemoyer, L. （2013）. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computa- tional Linguistics, 1（1）:49–62. Bengio, Y., Louradour, J., Collobert, R., and Weston, J. （2009）. Curriculum learning. In Proceedings of ICML, pages 41–48, Montreal, Canada. Boleda, G. and Erk, K. （2015）. Distributional semantic features as semantic primitives–or not. In Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches, pages 2–5, Stanford, CA. Bottou, L. （2014）. From machine learning to machine reasoning: an essay. Machine Learning, 94:133–149. Branavan, S., Silver, D., and Barzilay, R. （2012）. Learning to win by reading manuals in a Monte-Carlo framework. Journal of Artificial Intelligence Research, 43:661–704. Caruana, R. （1997）. Multitask learning. Machine Learning, 28:41–75. Chen, D. and Mooney, R. （2011）. Learning to interpret natural language naviga- tion instructions from observations. In Proceedings of AAAI, pages 859–865, San Francisco, CA. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. （2011）. Natural language processing （almost） from scratch. Journal of Machine Learning Research, 12:2493–2537. Elman, J. （1993）. Learning and development in neural networks: the importance of starting small. Cognition, 48:71–99. Erk, K. （2012）. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6（10）:635–653. Fodor, J. （1975）. The Language of Thought. Crowell Press, New York. Geman, D., Geman, S., Hallonquist, N., and Younes, L. （2015）. Visual Turing test for computer vision systems. Proceedings of the National Academy of Sciences, 112（12）:3618–3623. Genesereth, M., Love, N., and Pell, B. （2005）. General game playing: Overview of the AAAI competition. AI Magazine, 26（2）:62–72. Graves, A., Wayne, G., and Danihelka, I. （2014）. Neural turing machines. http: //arxiv.org/abs/1410.5401. Grefenstette, E., Hermann, K., Suleyman, M., and Blunsom, P. （2015）. Learning to transduce with unbounded memory. In Proceedings of NIPS, Montreal, Canada. In press. Haugeland, J. （1985）. Artificial Intelligence: The Very Idea. MIT Press, Cambridge, MA. Joulin, A. and Mikolov, T. （2015）. Inferring algorithmic patterns with stack- augmented recurrent nets. In Proceedings of NIPS, Montreal, Canada. In press. Kaelbling, L. P., Littman, M. L., and Moore, A. W. （1996）. Reinforcement learning: A survey. Journal of artificial intelligence research, pages 237–285. LeCun, Y., Bengio, Y., and Hinton, G. （2015）. Deep learning. Nature, 521:436–444. Levesque, H. J., Davis, E., and Morgenstern, L. （2012）. The Winograd schema challenge. In Proceedings of KR, pages 362–372, Rome, Italy. Louwerse, M. （2011）. Symbol interdependency in symbolic and embodied cognition. Topics in Cognitive Science, 3:273–302. MacMahon, M., Stankiewicz, B., and Kuipers, B. （2006）. Walk the talk: Connecting language, knowledge, and action in route instructions. In Proceedings of AAAI, pages 1475–1482, Boston, MA. Mikolov, T. （2013）. Incremental learning of algorithms. Unpublished manuscript. Mikolov, T., Chen, K., Corrado, G., and Dean, J. （2013）. E cient estimation of word representations in vector space. http://arxiv.org/abs/1301.3781/. Mitchell, T., Cohen, W., Hruschka, E., Talukdar, P., Betteridge, J., Carlson, A., Mishra, B., Gardner, M., Kisiel, B., Krishnamurthy, J., Lao, N., Mazaitis, K., Mohamed, T., Nakashole, N., Platanios, E., Ritter, A., Samadi, M., Settles, B., Wang, R., Wijaya, D., Gupta, A., Chen, X., Saparov, A., Greaves, M., and Welling, J. （2015）. Never-ending learning. In Proceedings of AAAI, pages 2302–2310, Austin, TX. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A., Veness, J., Bellemare, M., Graves, A., Riedmiller, M., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. （2015）. Human-level control through deep reinforcement learning. Nature, 518:529– 533. Morelli, R., Brown, M., Anselmi, D., Haberlandt, K., and Lloyd, D., editors （1992）. Minds, Brains, and Computers: Perspectives in Cognitive Science and Artificial Intelligence. Ablex, Norwood, NJ. Narasimhan, K., Kulkarni, T., and Barzilay, R. （2015）. Language understanding for text-based games using deep reinforcement learning. In Proceedings of EMNLP, pages 1–11, Lisbon, Portugal. Nwana, H. （1996）. Software agents: An overview. Knowledge Engineering Review, 11（2）:1–40. Poli, R., Langdon, W., McPhee, N., and Koza, J. （2008）. A field guide to genetic programming. http://www.gp-field-guide.org.uk. Prusinkiewicz, P. and Lindenmayer, A. （2012）. The algorithmic beauty of plants. Springer Science & Business Media. Ring, M. （1997）. CHILD: A first step towards continual learning. Machine Learning, 28:77–104. Russell, S. and Norvig, P. （2009）. Artificial Intelligence: A Modern Approach, 3d ed. Pearson Education, New York. Schmidhuber, J. （2004）. Optimal ordered problem solver. Machine Learning, 54（3）:211–254. Schmidhuber, J. （2015）. On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. http://arxiv.org/abs/1511.09249. Shoham, Y. and Leyton-Brown, K. （2009）. Multiagent Systems. Cambridge University Press, Cambridge. Silver, D., Yang, Q., and Li, L. （2013）. Lifelong machine learning systems: Beyond learning algorithms. In Proceedings of the AAAI Spring Symposium on Lifelong Machine Learning, pages 49–55, Stanford, CA. Solomono↵, R. J. （1964）. A formal theory of inductive inference. Part I. Information and control, 7（1）:1–22. Solomono↵, R. J. （1997）. The discovery of algorithmic probability. Journal of Com- puter and System Sciences, 55（1）:73–88. Solomono↵, R. J. （2002）. Progress in incremental machine learning. In NIPS Workshop on Universal Learning Algorithms and Optimal Search, Whistler, BC. Citeseer. Steels, L. （2003）. Social language learning. In Tokoro, M. and Steels, L., editors, The Future of Learning, pages 133–162. IOS, Amsterdam. Steels, L. （2005）. What triggers the emergence of grammar? In Proceedings of EELC, pages 143–150, Hatfield, UK. Sukhbaatar, S., Szlam, A., Synnaeve, G., Chintala, S., and Fergus, R. （2015）. Maze- Base: a sandbox for learning from games. http://arxiv.org/abs/1511.07401. Sutton, R. and Barto, A. （1998）. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA. Tenenbaum, J. （2015）. Cognitive foundations for knowledge representation in AI. Presented at the AAAI Spring Symposium on Knowledge Representation and Rea- soning. Thomason, J., Zhang, S., Mooney, R., and Stone, P. （2015）. Learning to interpret natural language commands through human-robot dialog. In Proceedings IJCAI, pages 1923–1929, Buenos Aires, Argentina. Thrun, S. and Pratt, L., editors （1997）. Learning to Learn. Kluwer, Dordrecht. Turing, A. （1950）. Computing machinery and intelligence. Mind, 59:433–460. Turney, P. and Pantel, P. （2010）. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188. Von Neumann, J., Burks, A. W., 等人 （1966）. Theory of self-reproducing automata. IEEE Transactions on Neural Networks, 5（1）:3–14. Weston, J., Bordes, A., Chopra, S., and Mikolov, T. （2015）. Towards AI-complete question answering: A set of prerequisite toy tasks. http://arxiv.org/abs/1502. 05698. Wikipedia （2015a）. Artificial human companion. https://en.wikipedia.org/ w/index.php?title=Artificial\_human\_companion&oldid=685507143. Accessed 15-October-2015. Wikipedia （2015b）. Interactive fiction. https://en.wikipedia.org/w/index.php? title=Interactive\_fiction&oldid=693926750. Accessed 19-December-2015. Wikipedia （2015c）. Turing test. https://en.wikipedia.org/w/index.php?title= Turing\_test&oldid=673582926. Accessed 30-July-2015. Winograd, T. （1971）. Procedures as a representation for data in a computer pro- gram for understanding natural language. Technical Report AI 235, Massachusetts Institute of Technology.